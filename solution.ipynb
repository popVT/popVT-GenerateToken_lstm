{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a59d7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data_utils import clean_text, prepare_data, train_test_val\n",
    "from src.next_token_dataset import NextTokenDataset\n",
    "from src.lstm_model import LSTMGenerateWord\n",
    "from src.lstm_train import model_train\n",
    "from src.eval_lstm import model_eval\n",
    "from src.eval_transformer_pipeline import evaluate_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262327a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tweets.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = pd.DataFrame({'text': lines})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8afca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено 3563 пропусков.\n",
      "Датасет предобработан.\n",
      "Разделение на трейн, валидацию и тест прошло успешно.\n",
      "Train: (1277548, 1)\n",
      "Val: (159693, 1)\n",
      "Test: (159694, 1)\n"
     ]
    }
   ],
   "source": [
    "prepare_data(data)\n",
    "train_test_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d83a108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')['text_clean'].tolist()\n",
    "val = pd.read_csv('data/val.csv')['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ebb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:100]\n",
    "val = val[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e8f396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0bde528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_token_id=50256):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a27401e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NextTokenDataset(train, tokenizer, max_length=20)\n",
    "val_dataset = NextTokenDataset(val, tokenizer, max_length=20)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07802aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fa9f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.4881\n",
      "Epoch 2/5, Loss: 5.5851\n",
      "Epoch 3/5, Loss: 5.3827\n",
      "Epoch 4/5, Loss: 5.2791\n",
      "Epoch 5/5, Loss: 5.2126\n",
      "Модель сохранена\n"
     ]
    }
   ],
   "source": [
    "model = model_train(train_dataloader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bef311aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM ROUGE-1: 0.0000\n",
      "LSTM ROUGE-2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "rouge1_lstm, rouge2_lstm = model_eval(model, val_dataloader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb255ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing prompts and targets.....\n",
      "Evaluating on 30 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DistilGPT2: 100%|██████████| 30/30 [00:08<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DistilGPT2 Evaluation Results (30 examples)\n",
      "============================================================\n",
      "ROUGE-1: 0.0454\n",
      "ROUGE-2: 0.0076\n",
      "\n",
      "============================================================\n",
      "Examples:\n",
      "============================================================\n",
      "\n",
      "Example 1:\n",
      "Prompt: ...peace\n",
      "Target:  good\n",
      "Generated: It is not that the same laws or practices as those\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Prompt: ...taking my rotten apple to\n",
      "Target:  the doc\n",
      "Generated: the point that I could see a little more of it.\n",
      "ROUGE-1: 0.154, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Prompt: ...having a gappyf\n",
      "Target: ringe day\n",
      "Generated: ag about how it works. It is a lot more complex\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Prompt: ...missing squints terribly ill\n",
      "Target:  always remember\n",
      "Generated: , and the rest of the people around you are just waiting\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Prompt: ...i dont feel good i want to\n",
      "Target:  go out ton\n",
      "Generated: see you guys play, i want to see you guys play,\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores, examples = evaluate_transformer(val_dataloader, tokenizer, device=device, max_examples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f80da879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "Промпт: i love\n",
      "Дополнение LSTM: i love you guys and i love you guys i love you guys i love you guys i love you\n",
      "Промпт: this person is\n",
      "Дополнение LSTM: this person is a little bit of a little bit of a little bit of a headache i hate it\n",
      "Промпт: she go\n",
      "Дополнение LSTM: she goin to bed i miss you guys too much i love you guys and i love you guys\n",
      "Промпт: we work\n",
      "Дополнение LSTM: we work on the new album i have to go to the gym and i have to go to work\n",
      "Промпт: i want\n",
      "Дополнение LSTM: i want to go to the beach and i have to go to work tomorrow i have to go to\n",
      "\n",
      "DistilGPT:\n",
      "Промпт: i love\n",
      "Дополнение DistilGPT: i love and hate.‬We‬g love\n",
      "Промпт: this person is\n",
      "Дополнение DistilGPT: this person is a good man, but you just can't do\n",
      "Промпт: she go\n",
      "Дополнение DistilGPT: she go to sleep after you're gone.”\n",
      "Промпт: we work\n",
      "Дополнение DistilGPT: we work on the “‘‘‘\n",
      "Промпт: i want\n",
      "Дополнение DistilGPT: i want to talk to them.››�\n"
     ]
    }
   ],
   "source": [
    "generator_DistilGPT = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"distilgpt2\", \n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Примеры промптов — начала фраз\n",
    "examples = [\n",
    "    \"i love\",\n",
    "    \"this person is\",\n",
    "    \"she go\",\n",
    "    \"we work\",\n",
    "    \"i want\"\n",
    "]\n",
    "print(\"LSTM:\")\n",
    "for prompt in examples:\n",
    "    generated = model.generate(tokenizer, prompt, max_length=20, device=device)\n",
    "    print(f\"Промпт: {prompt}\")\n",
    "    print(f\"Дополнение LSTM: {generated}\")\n",
    "\n",
    "print(\"\\nDistilGPT:\")\n",
    "\n",
    "for prompt in examples:\n",
    "    result = generator_DistilGPT(prompt, max_new_tokens=10, do_sample=True, temperature=0.8, top_k=50)\n",
    "    generated = result[0]['generated_text']\n",
    "    print(f\"Промпт: {prompt}\")\n",
    "    print(f\"Дополнение DistilGPT: {generated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbf6c3",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "1) У LSTM ROUGE-1 и ROUGE-2 равны 0.0. У DistilGPT ROUGE-1 и ROUGE-2 равны 0.0454 и 0.0076 соответственно.\n",
    "2) DistilGPT дополняет текст более логично, генерируя более связанные слова. LSTM с такой задачей справляется хуже.\n",
    "\n",
    "Итог: DistilGPT лучше подходит для задачи автодополения текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6cb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99909968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
