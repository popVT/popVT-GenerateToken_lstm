{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59d7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.data_utils import clean_text, prepare_data, train_test_val\n",
    "from src.next_token_dataset import NextTokenDataset\n",
    "from src.lstm_model import LSTMGenerateWord\n",
    "from src.lstm_train import model_train\n",
    "from src.eval_lstm import model_eval\n",
    "#from src.eval_transformer_pipeline import evaluate_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262327a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tweets.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = pd.DataFrame({'text': lines})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8afca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено 3563 пропусков.\n",
      "Датасет предобработан.\n",
      "Разделение на трейн, валидацию и тест прошло успешно.\n",
      "Train: (1277548, 1)\n",
      "Val: (159693, 1)\n",
      "Test: (159694, 1)\n"
     ]
    }
   ],
   "source": [
    "prepare_data(data)\n",
    "train_test_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83a108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')['text_clean'].tolist()\n",
    "val = pd.read_csv('data/val.csv')['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ebb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:100]\n",
    "val = val[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8f396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0bde528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_token_id=50256):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27401e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NextTokenDataset(train, tokenizer, max_length=20)\n",
    "val_dataset = NextTokenDataset(val, tokenizer, max_length=20)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07802aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa9f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 10.8261\n",
      "Epoch 2/5, Loss: 10.8020\n",
      "Epoch 3/5, Loss: 10.7724\n",
      "Epoch 4/5, Loss: 10.7317\n",
      "Epoch 5/5, Loss: 10.6677\n",
      "Модель сохранена\n"
     ]
    }
   ],
   "source": [
    "model = model_train(train_dataloader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef311aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM ROUGE-1: 0.0000\n",
      "LSTM ROUGE-2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "rouge1_lstm, rouge2_lstm = model_eval(model, val_dataloader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9c61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate_transformer(dataloader, tokenizer, device='cpu', max_examples=50, split_ratio=0.75):\n",
    "    \"\"\"\n",
    "    Оценка distilgpt2 с использованием pipeline\n",
    "    \"\"\"\n",
    "    # Определяем устройство\n",
    "    device_id = 0 if device == \"cuda\" else -1\n",
    "    print(f\"Using device: {device_id}\")\n",
    "    \n",
    "    # Загружаем модель через pipeline\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"distilgpt2\",\n",
    "        device=device_id,\n",
    "        tokenizer=tokenizer,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False  # возвращаем только сгенерированную часть\n",
    "    )\n",
    "\n",
    "    # Собираем промпты и целевые тексты\n",
    "    prompts = []\n",
    "    target_texts = []\n",
    "    full_texts = []\n",
    "\n",
    "    print(\"Preparing prompts and targets.....\")\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        for i in range(input_ids.size(0)):\n",
    "            if len(prompts) >= max_examples:\n",
    "                break\n",
    "                \n",
    "            # Получаем последовательность (игнорируем паддинг)\n",
    "            sequence = input_ids[i]\n",
    "            sequence = sequence[sequence != tokenizer.pad_token_id]\n",
    "            \n",
    "            #if len(sequence) < 20:  # Пропускаем слишком короткие\n",
    "                #continue\n",
    "                \n",
    "            # Разделяем на промпт (3/4) и таргет (1/4)\n",
    "            split_point = int(len(sequence) * split_ratio)\n",
    "            prompt_tokens = sequence[:split_point]\n",
    "            target_tokens = sequence[split_point:]\n",
    "            \n",
    "            #if len(target_tokens) < 5:  # Минимальная длина таргета\n",
    "                #continue\n",
    "                \n",
    "            # Декодируем\n",
    "            prompt_text = tokenizer.decode(prompt_tokens, skip_special_tokens=True)\n",
    "            target_text = tokenizer.decode(target_tokens, skip_special_tokens=True)\n",
    "            full_text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "            \n",
    "            prompts.append(prompt_text)\n",
    "            target_texts.append(target_text)\n",
    "            full_texts.append(full_text)\n",
    "            \n",
    "        if len(prompts) >= max_examples:\n",
    "            break\n",
    "\n",
    "    if not prompts:\n",
    "        print(\"Нет подходящих данных для оценки.\")\n",
    "        return None, []\n",
    "\n",
    "    print(f\"Evaluating on {len(prompts)} examples...\")\n",
    "    \n",
    "    # Инициализация ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    total_scores = {'rouge1': 0.0, 'rouge2': 0.0}\n",
    "    examples = []\n",
    "\n",
    "    # Генерация и оценка\n",
    "    for i, (prompt, target, full) in enumerate(tqdm(zip(prompts, target_texts, full_texts), \n",
    "                                                   desc=\"Evaluating DistilGPT2\", \n",
    "                                                   total=len(prompts))):\n",
    "        try:\n",
    "            # Генерация с различными параметрами\n",
    "            outputs = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=len(target.split()) + 10,  # Длина + запас\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            generated_text = outputs[0]['generated_text'].strip()\n",
    "            \n",
    "            # Вычисляем ROUGE между сгенерированной частью и таргетом\n",
    "            scores = scorer.score(target, generated_text)\n",
    "            \n",
    "            # Суммируем scores\n",
    "            for key in total_scores:\n",
    "                total_scores[key] += scores[key].fmeasure\n",
    "            \n",
    "            # Сохраняем примеры для вывода\n",
    "            if i < 5:  # Первые 5 примеров\n",
    "                examples.append({\n",
    "                    'prompt': prompt[-100:],  # Последние 100 символов промпта\n",
    "                    'target': target,\n",
    "                    'generated': generated_text,\n",
    "                    'rouge1': scores['rouge1'].fmeasure,\n",
    "                    'rouge2': scores['rouge2'].fmeasure\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error on example {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Вычисляем средние значения\n",
    "    count = len(prompts)\n",
    "    avg_scores = {key: total_scores[key] / count for key in total_scores}\n",
    "    \n",
    "    # Вывод результатов\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DistilGPT2 Evaluation Results ({count} examples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ROUGE-1: {avg_scores['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {avg_scores['rouge2']:.4f}\")\n",
    "    \n",
    "    # Вывод примеров\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Examples:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Prompt: ...{example['prompt']}\")\n",
    "        print(f\"Target: {example['target']}\")\n",
    "        print(f\"Generated: {example['generated']}\")\n",
    "        print(f\"ROUGE-1: {example['rouge1']:.3f}, ROUGE-2: {example['rouge2']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return avg_scores, examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb255ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing prompts and targets.....\n",
      "Evaluating on 30 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DistilGPT2:  63%|██████▎   | 19/30 [00:03<00:01,  5.97it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Evaluating DistilGPT2: 100%|██████████| 30/30 [00:04<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DistilGPT2 Evaluation Results (30 examples)\n",
      "============================================================\n",
      "ROUGE-1: 0.0552\n",
      "ROUGE-2: 0.0000\n",
      "\n",
      "============================================================\n",
      "Examples:\n",
      "============================================================\n",
      "\n",
      "Example 1:\n",
      "Prompt: ...peace\n",
      "Target:  good\n",
      "Generated: It's time to stop playing the card. I'm\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Prompt: ...taking my rotten apple to\n",
      "Target:  the doc\n",
      "Generated: be a friend to my sister.\n",
      "I‪m\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Prompt: ...having a gappyf\n",
      "Target: ringe day\n",
      "Generated: iddle.\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Prompt: ...missing squints terribly ill\n",
      "Target:  always remember\n",
      "Generated: \n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Prompt: ...i dont feel good i want to\n",
      "Target:  go out ton\n",
      "Generated: be the next one, i'm gonna be there as soon as\n",
      "ROUGE-1: 0.000, ROUGE-2: 0.000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores, examples = evaluate_transformer(val_dataloader, tokenizer, device=device, max_examples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f80da879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка автодополнения LSTM:\n",
      "Промпт: i love\n",
      "Дополнение LSTM: i love 630 630 the the the the the the the the the the the the the the the the\n",
      "Промпт: today is\n",
      "Дополнение LSTM: today is not not not not not not not not not the the the the the the the the the\n",
      "Промпт: i feel\n",
      "Дополнение LSTM: i feel 630 and the the the the the the the the the the the the the the the the\n",
      "Промпт: this is\n",
      "Дополнение LSTM: this is not not not not not not not not not the the the the the the the the the\n",
      "Промпт: i want\n",
      "Дополнение LSTM: i want 630 not not not not not not not not the the the the the the the the the\n",
      "\n",
      "Оценка автодополнения DistilGPT:\n",
      "Промпт: i love\n",
      "Дополнение DistilGPT: i love you. In an era of unprecedented political polarization,\n",
      "Промпт: today is\n",
      "Дополнение DistilGPT: today is a new concept to build on the original research done\n",
      "Промпт: i feel\n",
      "Дополнение DistilGPT: i feel as though they’re in the right place\n",
      "Промпт: this is\n",
      "Дополнение DistilGPT: this is going to be a fantastic story and I want to\n",
      "Промпт: i want\n",
      "Дополнение DistilGPT: i want to find a single black man you truly love.\n"
     ]
    }
   ],
   "source": [
    "generator_DistilGPT = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"distilgpt2\", \n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Примеры промптов — начала фраз\n",
    "examples = [\n",
    "    \"i love\",\n",
    "    \"today is\",\n",
    "    \"i feel\",\n",
    "    \"this is\",\n",
    "    \"i want\"\n",
    "]\n",
    "print(\"Оценка автодополнения LSTM:\")\n",
    "for prompt in examples:\n",
    "    generated = model.generate(tokenizer, prompt, max_length=20, device=device)\n",
    "    print(f\"Промпт: {prompt}\")\n",
    "    print(f\"Дополнение LSTM: {generated}\")\n",
    "print(\"\\nОценка автодополнения DistilGPT:\")\n",
    "for prompt in examples:\n",
    "    result = generator_DistilGPT(prompt, max_new_tokens=10, do_sample=True, temperature=0.8, top_k=50)\n",
    "    generated = result[0]['generated_text']\n",
    "    print(f\"Промпт: {prompt}\")\n",
    "    print(f\"Дополнение DistilGPT: {generated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6cb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99909968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637f757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20081b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d364d7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train_dataset: 1272634 примеров\n",
      "Размер val_dataset: 159106 примеров\n",
      "\n",
      "Первые 3 примера из train_dataset:\n",
      "\n",
      "Пример 0:\n",
      "Input IDs: [11195, 422, 670, 1110, 734, 19698, 257, 7534, 3052, 287, 1022, 2111, 284, 1334, 290, 21509, 617, 2568, 26645]\n",
      "Labels: [422, 670, 1110, 734, 19698, 257, 7534, 3052, 287, 1022, 2111, 284, 1334, 290, 21509, 617, 2568, 26645, 1660]\n",
      "Декодированный input: home from work day two updating a clients website in between trying to rest and regain some energy retaining\n",
      "Декодированный target:  from work day two updating a clients website in between trying to rest and regain some energy retaining water\n",
      "\n",
      "Пример 1:\n",
      "Input IDs: [10919, 389, 345, 991, 1804, 510, 1862]\n",
      "Labels: [389, 345, 991, 1804, 510, 1862, 10846]\n",
      "Декодированный input: what are you still doing up young\n",
      "Декодированный target:  are you still doing up young lady\n",
      "\n",
      "Пример 2:\n",
      "Input IDs: [361, 7722, 373, 281, 267, 6760, 291, 6332, 4686, 423, 1760, 35918, 1044, 6613, 428, 5041, 1312, 765, 284]\n",
      "Labels: [7722, 373, 281, 267, 6760, 291, 6332, 4686, 423, 1760, 35918, 1044, 6613, 428, 5041, 1312, 765, 284, 3960]\n",
      "Декодированный input: if drinking was an olympic sport id have done ireland proud this weekend i want to\n",
      "Декодированный target:  drinking was an olympic sport id have done ireland proud this weekend i want to cry\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим информацию о датасете\n",
    "print(f\"Размер train_dataset: {len(train_dataset)} примеров\")\n",
    "print(f\"Размер val_dataset: {len(val_dataset)} примеров\")\n",
    "\n",
    "# Посмотрим несколько примеров из датасета\n",
    "print(\"\\nПервые 3 примера из train_dataset:\")\n",
    "for i in range(min(3, len(train_dataset))):\n",
    "    sample = train_dataset[i]\n",
    "    print(f\"\\nПример {i}:\")\n",
    "    print(f\"Input IDs: {sample['input_ids']}\")\n",
    "    print(f\"Labels: {sample['labels']}\")\n",
    "    print(f\"Декодированный input: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)}\")\n",
    "    print(f\"Декодированный target: {tokenizer.decode(sample['labels'], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd0aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первый батч из train_dataloader:\n",
      "\n",
      "Батч 0:\n",
      "Input IDs shape: torch.Size([64, 19])\n",
      "Labels shape: torch.Size([64, 19])\n",
      "\n",
      "Input IDs:\n",
      "tensor([[ 6814,  7252,   321,  ...,  1381,   407,   257],\n",
      "        [ 3137,   925,   340,  ...,     0,     0,     0],\n",
      "        [   72,  1842,   345,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1820,  3329,   318,  ...,     0,     0,     0],\n",
      "        [   72,  8138,   479,  ...,   788,  1312,  3285],\n",
      "        [20342,     0,     0,  ...,     0,     0,     0]])\n",
      "\n",
      "Labels:\n",
      "tensor([[ 7252,   321,  3020,  ...,   407,   257,   922],\n",
      "        [  925,   340,   832,  ...,     0,     0,     0],\n",
      "        [ 1842,   345,  1165,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 3329,   318,  8805,  ...,     0,     0,     0],\n",
      "        [ 8138,   479,  5892,  ...,  1312,  3285, 23611],\n",
      "        [19462,     0,     0,  ...,     0,     0,     0]])\n",
      "\n",
      "Декодированные примеры:\n",
      "\n",
      "Пример 0 в батче:\n",
      "Input: 'daaaammmnnn and i wear a sz 9 soo dats not a'\n",
      "Target: 'aaammmnnn and i wear a sz 9 soo dats not a good'\n",
      "\n",
      "Пример 1 в батче:\n",
      "Input: 'just made it through 16 hours of maths in 2 days exams next!!!!!!'\n",
      "Target: ' made it through 16 hours of maths in 2 days exams next week!!!!!!'\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим первый батч из DataLoader\n",
    "print(\"\\nПервый батч из train_dataloader:\")\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    print(f\"\\nБатч {batch_idx}:\")\n",
    "    print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "    #print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    \n",
    "    print(f\"\\nInput IDs:\\n{batch['input_ids']}\")\n",
    "    print(f\"\\nLabels:\\n{batch['labels']}\")\n",
    "    #print(f\"\\nAttention mask:\\n{batch['attention_mask']}\")\n",
    "    \n",
    "    # Декодируем примеры из батча\n",
    "    print(f\"\\nДекодированные примеры:\")\n",
    "    for i in range(min(2, batch['input_ids'].shape[0])):  # Первые 2 примера\n",
    "        print(f\"\\nПример {i} в батче:\")\n",
    "        input_text = tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)\n",
    "        # Для labels игнорируем -100 (паддинг)\n",
    "        label_ids = [x for x in batch['labels'][i].tolist() if x != -100]\n",
    "        label_text = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Input: '{input_text}'\")\n",
    "        print(f\"Target: '{label_text}'\")\n",
    "        #print(f\"Attention mask: {batch['attention_mask'][i].tolist()}\")\n",
    "    \n",
    "    break  # Смотрим только первый батч"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2815a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Быстрая проверка DataLoader:\n",
      "Типы данных:\n",
      "  Input IDs: torch.int64, shape: torch.Size([64, 19])\n",
      "  Labels: torch.int64, shape: torch.Size([64, 19])\n",
      "\n",
      "Проверка на NaN:\n",
      "  Input IDs has NaN: False\n",
      "  Labels has NaN: False\n"
     ]
    }
   ],
   "source": [
    "# Быстрая проверка без деталей\n",
    "print(\"Быстрая проверка DataLoader:\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(f\"Типы данных:\")\n",
    "print(f\"  Input IDs: {batch['input_ids'].dtype}, shape: {batch['input_ids'].shape}\")\n",
    "print(f\"  Labels: {batch['labels'].dtype}, shape: {batch['labels'].shape}\")\n",
    "#print(f\"  Attention mask: {batch['attention_mask'].dtype}, shape: {batch['attention_mask'].shape}\")\n",
    "\n",
    "# Проверка на наличие NaN значений\n",
    "print(f\"\\nПроверка на NaN:\")\n",
    "print(f\"  Input IDs has NaN: {torch.isnan(batch['input_ids']).any()}\")\n",
    "print(f\"  Labels has NaN: {torch.isnan(batch['labels']).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32570d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 6814,  7252,   321,  ...,  1381,   407,   257],\n",
       "         [ 3137,   925,   340,  ...,     0,     0,     0],\n",
       "         [   72,  1842,   345,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 1820,  3329,   318,  ...,     0,     0,     0],\n",
       "         [   72,  8138,   479,  ...,   788,  1312,  3285],\n",
       "         [20342,     0,     0,  ...,     0,     0,     0]]),\n",
       " 'labels': tensor([[ 7252,   321,  3020,  ...,   407,   257,   922],\n",
       "         [  925,   340,   832,  ...,     0,     0,     0],\n",
       "         [ 1842,   345,  1165,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 3329,   318,  8805,  ...,     0,     0,     0],\n",
       "         [ 8138,   479,  5892,  ...,  1312,  3285, 23611],\n",
       "         [19462,     0,     0,  ...,     0,     0,     0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f4c9760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70b35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMGenerateWord(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(x)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def generate(self, tokenizer, prompt, max_length=20, device='cpu'):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            tokens = tokenizer.encode(prompt.lower(), return_tensors='pt').to(device)\n",
    "            generated = tokens.clone()\n",
    "\n",
    "            for _ in range(max_length - tokens.size(1)):\n",
    "                logits, _ = self.forward(generated)\n",
    "                next_token_logits = logits[:, -1, :]  # последний токен\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62b3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "#from src.lstm_model import LSTMGenerateWord\n",
    "\n",
    "model = LSTMGenerateWord(vocab_size=tokenizer.vocab_size).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8872caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "#from src.lstm_model import LSTMGenerateWord\n",
    "\n",
    "model = LSTMGenerateWord(vocab_size=tokenizer.vocab_size).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(input_ids)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d3c1f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m target_ids[target_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]  \u001b[38;5;66;03m# Убираем паддинг\u001b[39;00m\n\u001b[0;32m     24\u001b[0m target \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(target_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpromt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m results \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mscore(target, generated)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#results = rouge.compute(predictions=target, references=generated, use_stemmer=True)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mLSTMGenerateWord.generate\u001b[1;34m(self, tokenizer, prompt, max_length, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m generated \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length \u001b[38;5;241m-\u001b[39m tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 33\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# последний токен\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mLSTMGenerateWord.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, hidden)\n",
      "File \u001b[1;32mc:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Никита\\Desktop\\Курс по DL (Яндекс)\\Спринт2_RNN\\.sprint2\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "#rouge = evaluate.load('rouge')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "\n",
    "model.eval()\n",
    "total_rouge1 = 0\n",
    "total_rouge2 = 0\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            seg_len = int(input_ids.size(0) * 0.75)\n",
    "\n",
    "            promt_ids = input_ids[i][: seg_len]\n",
    "            promt = tokenizer.decode(promt_ids, skip_special_tokens=True)\n",
    "\n",
    "            target_ids = labels[i][seg_len:]\n",
    "            target_ids = target_ids[target_ids != -100]  # Убираем паддинг\n",
    "            target = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "            generated = model.generate(tokenizer, promt, max_length=30, device=device)\n",
    "            \n",
    "            results = scorer.score(target, generated)\n",
    "\n",
    "            #results = rouge.compute(predictions=target, references=generated, use_stemmer=True)\n",
    "            total_rouge1 += results['rouge1'].fmeasure\n",
    "            total_rouge2 += results['rouge2'].fmeasure\n",
    "            count += 1\n",
    "\n",
    "print(f\"LSTM ROUGE-1: {total_rouge1/count:.4f}\")\n",
    "print(f\"LSTM ROUGE-2: {total_rouge2/count:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0518a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sprint2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
